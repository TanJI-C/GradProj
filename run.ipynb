{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import wandb\n",
    "from argparse import Namespace\n",
    "import datetime\n",
    "\n",
    "import util as tu\n",
    "import model as tm\n",
    "from setup import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace (\n",
    "    project_name = \"TanJI\",\n",
    "    test_database_ids = [13],\n",
    "    random_seed = 114514,\n",
    "    batch_size = 1000,\n",
    "    accelerator = 'cpu',\n",
    "    progress_bar = True,\n",
    "    lr = 0.001,\n",
    "    dropout = 0.2,\n",
    "    epochs = 200,\n",
    "    pad_length = 20,\n",
    "    node_length = 18,\n",
    "    hidden_dim = 192,\n",
    "    output_dim = 1,\n",
    "    newpath = './results/full/cost/',\n",
    "    to_predict = 'cost',\n",
    "    dataset_file_name = 'alldataset.pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nested Loop': 0, 'Merge Join': 1, 'BitmapAnd': 2, 'Sort': 3, 'Hash Join': 4, 'Bitmap Index Scan': 5, 'Seq Scan': 6, 'Bitmap Heap Scan': 7, 'Materialize': 8, 'Hash': 9, 'Index Scan': 10, 'Gather Merge': 11, 'Gather': 12}\n",
      "{'Actual Total Time': RobustScaler(), 'Plan Rows': RobustScaler(), 'Total Cost': RobustScaler()}\n"
     ]
    }
   ],
   "source": [
    "# with open(path.join(ROOT_DIR, 'data', 'workload1', 'statistics.json')) as file:\n",
    "#     feature_statistics = json.load(file)\n",
    "with open(path.join(ROOT_DIR, 'data', 'imdb', 'statistics.json')) as file:\n",
    "    feature_statistics = json.load(file)\n",
    "encoder = tu.Encoder(feature_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 存储类的实例\n",
    "def save_instance(instance, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(instance, file)\n",
    "\n",
    "# 加载存储的实例\n",
    "def load_instance(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        instance = pickle.load(file)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 1, 360]) torch.Size([100000, 20, 20]) torch.Size([100000, 1]) torch.Size([100000, 1])\n"
     ]
    }
   ],
   "source": [
    "# for zero-shot data\n",
    "files_name = []\n",
    "\n",
    "# for wk_item in workloads:\n",
    "#     files_name.append(path.join(ROOT_DIR, 'data', 'workload1', wk_item + '_filted.json'))\n",
    "for i in range(20):\n",
    "    files_name.append(path.join(ROOT_DIR, 'data', 'imdb', 'plan_and_cost', 'train_plan_part{}.csv'.format(i)))\n",
    "\n",
    "# data_file_name = path.join(ROOT_DIR, 'data', 'workload1', config.dataset_file_name)\n",
    "data_file_name = path.join(ROOT_DIR, 'data', 'imdb', config.dataset_file_name)\n",
    "\n",
    "if os.path.exists(data_file_name):\n",
    "    alldataset = load_instance(data_file_name)\n",
    "else:\n",
    "    # dict_list = tu.Encoder.format_workload(files_name)\n",
    "    dict_list = tu.Encoder.format_imdb(files_name)\n",
    "    alldataset = tu.PlanTreeDataSet(dict_list, encoder)\n",
    "    save_instance(alldataset, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4 4\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = tu.get_dataloader(alldataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanji\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240319_001257-nmuq8aby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanji/TanJI/runs/nmuq8aby' target=\"_blank\">2024-03-19 00:12:53</a></strong> to <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">https://wandb.ai/tanji/TanJI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanji/TanJI/runs/nmuq8aby' target=\"_blank\">https://wandb.ai/tanji/TanJI/runs/nmuq8aby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "model = tm.TanJI(config)\n",
    "logger = pl.loggers.WandbLogger(project=config.project_name, name=nowtime)\n",
    "logger.log_hyperparams(config.__dict__)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(ROOT_DIR, \"checkpoints\"),\n",
    "    filename=\"DACE\",\n",
    ")\n",
    "trainer = tm.PLTrainer(\n",
    "    accelerator=config.accelerator,\n",
    "    enable_progress_bar=config.progress_bar,\n",
    "    enable_model_summary=config.progress_bar,\n",
    "    max_epochs=config.epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = PLTrainer(accelerator=\"cpu\", max_epochs=50, logger=wandb_logger)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "result = trainer.test(model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
