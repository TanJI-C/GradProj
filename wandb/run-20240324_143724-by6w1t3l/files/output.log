GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\callbacks\model_checkpoint.py:617: UserWarning: Checkpoint directory D:\TanJI\Documents\document\Work\Research\DB4AI\2024届本科毕业论文（设计）工作指南(1)\TanJI\checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
  | Name               | Type               | Params
----------------------------------------------------------
0 | tranformer_encoder | TransformerEncoder | 10.2 K
1 | mlp                | Sequential         | 62.2 K
2 | sigmoid            | Sigmoid            | 0
----------------------------------------------------------
72.4 K    Trainable params
0         Non-trainable params
72.4 K    Total params
0.290     Total estimated model params size (MB)
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\aten\src\ATen\native\transformers\attention.cpp:152.)
  return torch._native_multi_head_attention(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(























































Epoch 6: 100%|██████████| 86/86 [00:14<00:00,  6.14it/s, v_num=1t3l]





Epoch 7: 100%|██████████| 86/86 [00:13<00:00,  6.31it/s, v_num=1t3l]



























Epoch 11: 100%|██████████| 86/86 [00:14<00:00,  6.13it/s, v_num=1t3l]























Epoch 14: 100%|██████████| 86/86 [00:15<00:00,  5.71it/s, v_num=1t3l]





















Epoch 17: 100%|██████████| 86/86 [00:14<00:00,  6.10it/s, v_num=1t3l]






















Epoch 20: 100%|██████████| 86/86 [00:15<00:00,  5.53it/s, v_num=1t3l]







Epoch 21: 100%|██████████| 86/86 [00:15<00:00,  5.71it/s, v_num=1t3l]
































































Epoch 30: 100%|██████████| 86/86 [00:14<00:00,  5.92it/s, v_num=1t3l]




























Epoch 34: 100%|██████████| 86/86 [00:14<00:00,  6.14it/s, v_num=1t3l]




























Epoch 38: 100%|██████████| 86/86 [00:14<00:00,  6.09it/s, v_num=1t3l]



































Epoch 43: 100%|██████████| 86/86 [00:14<00:00,  6.08it/s, v_num=1t3l]






Epoch 44: 100%|██████████| 86/86 [00:13<00:00,  6.28it/s, v_num=1t3l]






Epoch 45: 100%|██████████| 86/86 [00:13<00:00,  6.26it/s, v_num=1t3l]






Epoch 46: 100%|██████████| 86/86 [00:13<00:00,  6.18it/s, v_num=1t3l]
































































Epoch 55: 100%|██████████| 86/86 [00:14<00:00,  5.96it/s, v_num=1t3l]














Epoch 57: 100%|██████████| 86/86 [00:14<00:00,  5.78it/s, v_num=1t3l]














Epoch 59: 100%|██████████| 86/86 [00:14<00:00,  5.98it/s, v_num=1t3l]




























Epoch 63: 100%|██████████| 86/86 [00:14<00:00,  6.03it/s, v_num=1t3l]





















Epoch 66: 100%|██████████| 86/86 [00:13<00:00,  6.17it/s, v_num=1t3l]






Epoch 67: 100%|██████████| 86/86 [00:13<00:00,  6.23it/s, v_num=1t3l]














Epoch 69: 100%|██████████| 86/86 [00:14<00:00,  5.89it/s, v_num=1t3l]





















































Epoch 76: 100%|██████████| 86/86 [00:14<00:00,  5.99it/s, v_num=1t3l]


























































Epoch 83: 100%|██████████| 86/86 [00:15<00:00,  5.39it/s, v_num=1t3l]





















Epoch 86: 100%|██████████| 86/86 [00:14<00:00,  5.94it/s, v_num=1t3l]





















Epoch 89: 100%|██████████| 86/86 [00:14<00:00,  6.00it/s, v_num=1t3l]





















Epoch 92: 100%|██████████| 86/86 [00:14<00:00,  6.08it/s, v_num=1t3l]






































































Epoch 102: 100%|██████████| 86/86 [00:14<00:00,  6.09it/s, v_num=1t3l]





















Epoch 105: 100%|██████████| 86/86 [00:14<00:00,  6.03it/s, v_num=1t3l]





















Epoch 108: 100%|██████████| 86/86 [00:14<00:00,  6.10it/s, v_num=1t3l]














Epoch 110: 100%|██████████| 86/86 [00:15<00:00,  5.51it/s, v_num=1t3l]







Epoch 111: 100%|██████████| 86/86 [00:15<00:00,  5.56it/s, v_num=1t3l]




































Epoch 116: 100%|██████████| 86/86 [00:14<00:00,  6.06it/s, v_num=1t3l]




































Epoch 121: 100%|██████████| 86/86 [00:14<00:00,  6.09it/s, v_num=1t3l]














Epoch 123: 100%|██████████| 86/86 [00:14<00:00,  5.86it/s, v_num=1t3l]







Epoch 124: 100%|██████████| 86/86 [00:15<00:00,  5.43it/s, v_num=1t3l]






















Epoch 127: 100%|██████████| 86/86 [00:15<00:00,  5.73it/s, v_num=1t3l]






















Epoch 130: 100%|██████████| 86/86 [00:15<00:00,  5.52it/s, v_num=1t3l]

















































Epoch 136: 100%|██████████| 86/86 [00:16<00:00,  5.20it/s, v_num=1t3l]















Epoch 138: 100%|██████████| 86/86 [00:15<00:00,  5.44it/s, v_num=1t3l]




































Epoch 143: 100%|██████████| 86/86 [00:14<00:00,  6.09it/s, v_num=1t3l]






















Epoch 146: 100%|██████████| 86/86 [00:14<00:00,  5.79it/s, v_num=1t3l]
















































Epoch 152: 100%|██████████| 86/86 [00:15<00:00,  5.68it/s, v_num=1t3l]







Epoch 153: 100%|██████████| 86/86 [00:15<00:00,  5.56it/s, v_num=1t3l]







Epoch 154: 100%|██████████| 86/86 [00:14<00:00,  5.75it/s, v_num=1t3l]






























Epoch 158: 100%|██████████| 86/86 [00:14<00:00,  5.94it/s, v_num=1t3l]














Epoch 160: 100%|██████████| 86/86 [00:14<00:00,  5.86it/s, v_num=1t3l]






























Epoch 164: 100%|██████████| 86/86 [00:14<00:00,  6.03it/s, v_num=1t3l]














Epoch 166: 100%|██████████| 86/86 [00:14<00:00,  6.01it/s, v_num=1t3l]




























Epoch 170: 100%|██████████| 86/86 [00:14<00:00,  6.06it/s, v_num=1t3l]





















Epoch 173: 100%|██████████| 86/86 [00:14<00:00,  5.92it/s, v_num=1t3l]














Epoch 175: 100%|██████████| 86/86 [00:14<00:00,  5.80it/s, v_num=1t3l]














Epoch 177: 100%|██████████| 86/86 [00:15<00:00,  5.63it/s, v_num=1t3l]














Epoch 179: 100%|██████████| 86/86 [00:14<00:00,  5.89it/s, v_num=1t3l]














Epoch 181: 100%|██████████| 86/86 [00:15<00:00,  5.67it/s, v_num=1t3l]




























Epoch 185: 100%|██████████| 86/86 [00:14<00:00,  6.09it/s, v_num=1t3l]























































































Epoch 197: 100%|██████████| 86/86 [00:14<00:00,  6.03it/s, v_num=1t3l]














Epoch 199: 100%|██████████| 86/86 [00:14<00:00,  6.10it/s, v_num=1t3l]
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:00, 17.70it/s]

