{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import wandb\n",
    "from argparse import Namespace\n",
    "import datetime\n",
    "\n",
    "import util as tu\n",
    "import model as tm\n",
    "from setup import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Namespace (\n",
    "    project_name = \"TanJI\",\n",
    "    test_database_ids = [13],\n",
    "    random_seed = 114514,\n",
    "    batch_size = 1000,\n",
    "    accelerator = 'cpu',\n",
    "    progress_bar = True,\n",
    "    lr = 0.001,\n",
    "    dropout = 0.2,\n",
    "    epochs = 100,\n",
    "    pad_length = 20,\n",
    "    node_length = 19,\n",
    "    hidden_dim = 192,\n",
    "    output_dim = 1,\n",
    "    newpath = './results/full/cost/',\n",
    "    to_predict = 'cost',\n",
    "    dataset_file_name = 'alldataset.pkl',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Index Only Scan': 0, 'Index Scan': 1, 'Seq Scan': 2, 'Hash Join': 3, 'Bitmap Index Scan': 4, 'Result': 5, 'Merge Join': 6, 'Limit': 7, 'Aggregate': 8, 'Sort': 9, 'Hash': 10, 'Bitmap Heap Scan': 11, 'Nested Loop': 12, 'Materialize': 13, 'BitmapOr': 14, 'Memoize': 15, 'Gather': 16, 'Gather Merge': 17, 'BitmapAnd': 18}\n",
      "{'Actual Total Time': RobustScaler(), 'Plan Rows': RobustScaler(), 'Total Cost': RobustScaler()}\n"
     ]
    }
   ],
   "source": [
    "with open(path.join(ROOT_DIR, 'data', 'workload1', 'statistics.json')) as file:\n",
    "    feature_statistics = json.load(file)\n",
    "# with open(path.join(ROOT_DIR, 'data', 'imdb', 'statistics.json')) as file:\n",
    "#     feature_statistics = json.load(file)\n",
    "configs.node_length = len(feature_statistics['node_types']['value_dict']) + 2\n",
    "encoder = tu.Encoder(feature_statistics, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 存储类的实例\n",
    "def save_instance(instance, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(instance, file)\n",
    "\n",
    "# 加载存储的实例\n",
    "def load_instance(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        instance = pickle.load(file)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200000, 1, 420]) torch.Size([200000, 20, 20]) torch.Size([200000, 20]) torch.Size([200000, 20])\n"
     ]
    }
   ],
   "source": [
    "# for zero-shot data\n",
    "files_name = []\n",
    "\n",
    "for wk_item in workloads:\n",
    "    files_name.append(path.join(ROOT_DIR, 'data', 'workload1', wk_item + '_filted.json'))\n",
    "# for i in range(20):\n",
    "#     files_name.append(path.join(ROOT_DIR, 'data', 'imdb', 'plan_and_cost', 'train_plan_part{}.csv'.format(i)))\n",
    "\n",
    "data_file_name = path.join(ROOT_DIR, 'data', 'workload1', configs.dataset_file_name)\n",
    "# data_file_name = path.join(ROOT_DIR, 'data', 'imdb', configs.dataset_file_name)\n",
    "\n",
    "if os.path.exists(data_file_name):\n",
    "    alldataset = load_instance(data_file_name)\n",
    "else:\n",
    "    dict_list = tu.Encoder.format_workload(files_name)\n",
    "    # dict_list = tu.Encoder.format_imdb(files_name)\n",
    "    alldataset = tu.PlanTreeDataSet(dict_list, encoder)\n",
    "    save_instance(alldataset, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4 4\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = tu.get_dataloader(alldataset, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanji\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240324_164655-3f5e5gb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanji/TanJI/runs/3f5e5gb8' target=\"_blank\">2024-03-24 16:46:51</a></strong> to <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">https://wandb.ai/tanji/TanJI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanji/TanJI/runs/3f5e5gb8' target=\"_blank\">https://wandb.ai/tanji/TanJI/runs/3f5e5gb8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "model = tm.TanJI(configs)\n",
    "logger = pl.loggers.WandbLogger(project=configs.project_name, name=nowtime)\n",
    "logger.log_hyperparams(configs.__dict__)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.path.join(ROOT_DIR, \"checkpoints\"),\n",
    "    filename=\"TanJI\",\n",
    ")\n",
    "trainer = tm.PLTrainer(\n",
    "    accelerator=configs.accelerator,\n",
    "    enable_progress_bar=configs.progress_bar,\n",
    "    enable_model_summary=configs.progress_bar,\n",
    "    max_epochs=configs.epochs,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:617: UserWarning: Checkpoint directory D:\\TanJI\\Documents\\document\\Work\\Research\\DB4AI\\2024届本科毕业论文（设计）工作指南(1)\\TanJI\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name               | Type               | Params\n",
      "----------------------------------------------------------\n",
      "0 | tranformer_encoder | TransformerEncoder | 10.2 K\n",
      "1 | mlp                | Sequential         | 62.2 K\n",
      "2 | sigmoid            | Sigmoid            | 0     \n",
      "----------------------------------------------------------\n",
      "72.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "72.4 K    Total params\n",
      "0.290     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 171/171 [00:28<00:00,  5.98it/s, v_num=5gb8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 171/171 [00:28<00:00,  5.98it/s, v_num=5gb8]\n"
     ]
    }
   ],
   "source": [
    "# trainer = PLTrainer(accelerator=\"cpu\", max_epochs=50, logger=wandb_logger)\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "result = trainer.test(model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
