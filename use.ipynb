{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\TanJI\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from argparse import Namespace\n",
    "import datetime\n",
    "\n",
    "import util as tu\n",
    "import model as tm\n",
    "from setup import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Namespace (\n",
    "    project_name = \"TanJI\",\n",
    "    test_database_ids = [13],\n",
    "    random_seed = 114514,\n",
    "    batch_size = 1000,\n",
    "    accelerator = 'cpu',\n",
    "    progress_bar = True,\n",
    "    lr = 0.001,\n",
    "    dropout = 0.2,\n",
    "    epochs = 100,\n",
    "    pad_length = 20,\n",
    "    node_length = 19,\n",
    "    hidden_dim = 192,\n",
    "    output_dim = 1,\n",
    "    newpath = './results/full/cost/',\n",
    "    to_predict = 'cost',\n",
    "    dataset_file_name = 'alldataset.pkl',\n",
    "\n",
    "    test_data_name = 'accidents'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Index Only Scan': 0, 'Index Scan': 1, 'Seq Scan': 2, 'Hash Join': 3, 'Bitmap Index Scan': 4, 'Result': 5, 'Merge Join': 6, 'Limit': 7, 'Aggregate': 8, 'Sort': 9, 'Hash': 10, 'Bitmap Heap Scan': 11, 'Nested Loop': 12, 'Materialize': 13, 'BitmapOr': 14, 'Memoize': 15, 'Gather': 16, 'Gather Merge': 17, 'BitmapAnd': 18}\n",
      "{'Actual Total Time': RobustScaler(), 'Plan Rows': RobustScaler(), 'Total Cost': RobustScaler()}\n"
     ]
    }
   ],
   "source": [
    "with open(path.join(ROOT_DIR, 'data', 'workload1', 'statistics.json')) as file:\n",
    "    feature_statistics = json.load(file)\n",
    "# with open(path.join(ROOT_DIR, 'data', 'imdb', 'statistics.json')) as file:\n",
    "#     feature_statistics = json.load(file)\n",
    "configs.node_length = len(feature_statistics['node_types']['value_dict']) + 2\n",
    "encoder = tu.Encoder(feature_statistics, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanji\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240403_173645-fc8pyugr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanji/TanJI/runs/fc8pyugr' target=\"_blank\">2024-04-03 17:36:41test</a></strong> to <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanji/TanJI' target=\"_blank\">https://wandb.ai/tanji/TanJI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanji/TanJI/runs/fc8pyugr' target=\"_blank\">https://wandb.ai/tanji/TanJI/runs/fc8pyugr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "model = tm.TanJI(configs)\n",
    "model_dict = torch.load(\n",
    "    os.path.join(ROOT_DIR, 'checkpoints', 'TanJI.ckpt')\n",
    ")\n",
    "model.load_state_dict(model_dict[\"state_dict\"])\n",
    "\n",
    "logger = pl.loggers.WandbLogger(project=configs.project_name, name=nowtime+'test')\n",
    "logger.log_hyperparams(configs.__dict__)\n",
    "trainer = tm.PLTrainer(\n",
    "    accelerator=configs.accelerator,\n",
    "    enable_progress_bar=configs.progress_bar,\n",
    "    enable_model_summary=configs.progress_bar,\n",
    "    max_epochs=configs.epochs,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 存储类的实例\n",
    "# def save_instance(instance, filename):\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(instance, file)\n",
    "\n",
    "# # 加载存储的实例\n",
    "# def load_instance(filename):\n",
    "#     with open(filename, 'rb') as file:\n",
    "#         instance = pickle.load(file)\n",
    "#     return instance\n",
    "# # for zero-shot data\n",
    "# files_name = []\n",
    "\n",
    "# for wk_item in workloads:\n",
    "#     files_name.append(path.join(ROOT_DIR, 'data', 'workload1', wk_item + '_filted.json'))\n",
    "# # for i in range(20):\n",
    "# #     files_name.append(path.join(ROOT_DIR, 'data', 'imdb', 'plan_and_cost', 'train_plan_part{}.csv'.format(i)))\n",
    "\n",
    "# data_file_name = path.join(ROOT_DIR, 'data', 'workload1', configs.dataset_file_name)\n",
    "# # data_file_name = path.join(ROOT_DIR, 'data', 'imdb', configs.dataset_file_name)\n",
    "\n",
    "# if os.path.exists(data_file_name):\n",
    "#     alldataset = load_instance(data_file_name)\n",
    "# else:\n",
    "#     dict_list = tu.Encoder.format_workload(files_name)\n",
    "#     # dict_list = tu.Encoder.format_imdb(files_name)\n",
    "#     alldataset = tu.PlanTreeDataSet(dict_list, encoder)\n",
    "#     save_instance(alldataset, data_file_name)\n",
    "\n",
    "# train_dataloader, val_dataloader, test_dataloader = tu.get_dataloader(alldataset, configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n",
      "torch.Size([10000, 1, 420]) torch.Size([10000, 20, 20]) torch.Size([10000, 20]) torch.Size([10000, 20])\n"
     ]
    }
   ],
   "source": [
    "# workload 1\n",
    "\n",
    "for wk_item in workloads:\n",
    "    files_name = [path.join(ROOT_DIR, 'data', 'workload1', wk_item + '_filted.json')]\n",
    "\n",
    "    dict_list = tu.Encoder.format_workload(files_name)\n",
    "    alldataset = tu.PlanTreeDataSet(dict_list, encoder)\n",
    "\n",
    "    test_data = []\n",
    "    for plan_meta in alldataset:\n",
    "        test_data.append(plan_meta[:-1])        \n",
    "    bs = configs.batch_size\n",
    "    test_dataloader = DataLoader(dataset=test_data, batch_size=bs, shuffle=True)\n",
    "\n",
    "    result = trainer.test(model, dataloaders=test_dataloader, test_data_name=wk_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数数量: 72418\n"
     ]
    }
   ],
   "source": [
    "# # workload 3\n",
    "# test_workloads = ['job-light']\n",
    "# files_name = []\n",
    "# for tw in test_workloads:\n",
    "#     files_name.append(path.join(ROOT_DIR, 'data', 'workload2', tw + '_plans.json'))\n",
    "\n",
    "# dict_list = tu.Encoder.format_imdb_test(files_name)\n",
    "# alldataset = tu.PlanTreeDataSet(dict_list, encoder)\n",
    "\n",
    "# test_data = []\n",
    "# for plan_meta in alldataset:\n",
    "#     test_data.append(plan_meta[:-1])        \n",
    "# bs = configs.batch_size\n",
    "# test_dataloader = DataLoader(dataset=test_data, batch_size=bs, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# result = trainer.test(model, dataloaders=test_dataloader)\n",
    "\n",
    "# 计算模型参数数量\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"模型参数数量:\", num_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
