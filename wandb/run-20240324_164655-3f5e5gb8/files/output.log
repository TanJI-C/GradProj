GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\callbacks\model_checkpoint.py:617: UserWarning: Checkpoint directory D:\TanJI\Documents\document\Work\Research\DB4AI\2024届本科毕业论文（设计）工作指南(1)\TanJI\checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
  | Name               | Type               | Params
----------------------------------------------------------
0 | tranformer_encoder | TransformerEncoder | 10.2 K
1 | mlp                | Sequential         | 62.2 K
2 | sigmoid            | Sigmoid            | 0
----------------------------------------------------------
72.4 K    Trainable params
0         Non-trainable params
72.4 K    Total params
0.290     Total estimated model params size (MB)
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.
  rank_zero_warn(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\aten\src\ATen\native\transformers\attention.cpp:152.)
  return torch._native_multi_head_attention(
c:\Users\TanJI\AppData\Local\Programs\Python\Python310\lib\site-packages\lightning\pytorch\trainer\connectors\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(







































Epoch 2: 100%|██████████| 171/171 [00:27<00:00,  6.31it/s, v_num=5gb8]



























Epoch 4: 100%|██████████| 171/171 [00:26<00:00,  6.40it/s, v_num=5gb8]













Epoch 5: 100%|██████████| 171/171 [00:26<00:00,  6.41it/s, v_num=5gb8]








































Epoch 8: 100%|██████████| 171/171 [00:26<00:00,  6.38it/s, v_num=5gb8]













Epoch 9: 100%|██████████| 171/171 [00:26<00:00,  6.36it/s, v_num=5gb8]



















































































Epoch 15: 100%|██████████| 171/171 [00:27<00:00,  6.21it/s, v_num=5gb8]




































































Epoch 20: 100%|██████████| 171/171 [00:26<00:00,  6.33it/s, v_num=5gb8]













Epoch 21: 100%|██████████| 171/171 [00:27<00:00,  6.32it/s, v_num=5gb8]













Epoch 22: 100%|██████████| 171/171 [00:26<00:00,  6.35it/s, v_num=5gb8]


























Epoch 24: 100%|██████████| 171/171 [00:26<00:00,  6.37it/s, v_num=5gb8]













Epoch 25: 100%|██████████| 171/171 [00:26<00:00,  6.42it/s, v_num=5gb8]






















































Epoch 29: 100%|██████████| 171/171 [00:26<00:00,  6.39it/s, v_num=5gb8]













Epoch 30: 100%|██████████| 171/171 [00:26<00:00,  6.35it/s, v_num=5gb8]













Epoch 31: 100%|██████████| 171/171 [00:26<00:00,  6.37it/s, v_num=5gb8]


















































































Epoch 37: 100%|██████████| 171/171 [00:26<00:00,  6.39it/s, v_num=5gb8]









































Epoch 40: 100%|██████████| 171/171 [00:27<00:00,  6.30it/s, v_num=5gb8]













Epoch 41: 100%|██████████| 171/171 [00:26<00:00,  6.36it/s, v_num=5gb8]















Epoch 42: 100%|██████████| 171/171 [00:32<00:00,  5.26it/s, v_num=5gb8]





























Epoch 44: 100%|██████████| 171/171 [00:28<00:00,  6.05it/s, v_num=5gb8]










































Epoch 47: 100%|██████████| 171/171 [00:28<00:00,  6.10it/s, v_num=5gb8]




























Epoch 49: 100%|██████████| 171/171 [00:28<00:00,  6.06it/s, v_num=5gb8]














Epoch 50: 100%|██████████| 171/171 [00:28<00:00,  5.96it/s, v_num=5gb8]














Epoch 51: 100%|██████████| 171/171 [00:29<00:00,  5.84it/s, v_num=5gb8]

























































Epoch 55: 100%|██████████| 171/171 [00:28<00:00,  6.07it/s, v_num=5gb8]




























Epoch 57: 100%|██████████| 171/171 [00:27<00:00,  6.17it/s, v_num=5gb8]













Epoch 58: 100%|██████████| 171/171 [00:27<00:00,  6.18it/s, v_num=5gb8]




























Epoch 60: 100%|██████████| 171/171 [00:27<00:00,  6.15it/s, v_num=5gb8]













Epoch 61: 100%|██████████| 171/171 [00:27<00:00,  6.17it/s, v_num=5gb8]













Epoch 62: 100%|██████████| 171/171 [00:27<00:00,  6.22it/s, v_num=5gb8]










































Epoch 65: 100%|██████████| 171/171 [00:27<00:00,  6.15it/s, v_num=5gb8]










































Epoch 68: 100%|██████████| 171/171 [00:27<00:00,  6.22it/s, v_num=5gb8]













Epoch 69: 100%|██████████| 171/171 [00:27<00:00,  6.22it/s, v_num=5gb8]













Epoch 70: 100%|██████████| 171/171 [00:27<00:00,  6.20it/s, v_num=5gb8]






































































Epoch 75: 100%|██████████| 171/171 [00:27<00:00,  6.15it/s, v_num=5gb8]



























Epoch 77: 100%|██████████| 171/171 [00:27<00:00,  6.22it/s, v_num=5gb8]




























Epoch 79: 100%|██████████| 171/171 [00:27<00:00,  6.16it/s, v_num=5gb8]













Epoch 80: 100%|██████████| 171/171 [00:27<00:00,  6.23it/s, v_num=5gb8]













Epoch 81: 100%|██████████| 171/171 [00:27<00:00,  6.22it/s, v_num=5gb8]













Epoch 82: 100%|██████████| 171/171 [00:27<00:00,  6.24it/s, v_num=5gb8]













Epoch 83: 100%|██████████| 171/171 [00:27<00:00,  6.23it/s, v_num=5gb8]













Epoch 84: 100%|██████████| 171/171 [00:27<00:00,  6.25it/s, v_num=5gb8]













Epoch 85: 100%|██████████| 171/171 [00:27<00:00,  6.24it/s, v_num=5gb8]




















































































Epoch 91: 100%|██████████| 171/171 [00:27<00:00,  6.21it/s, v_num=5gb8]













Epoch 92: 100%|██████████| 171/171 [00:27<00:00,  6.19it/s, v_num=5gb8]










































Epoch 95: 100%|██████████| 171/171 [00:27<00:00,  6.20it/s, v_num=5gb8]













Epoch 96: 100%|██████████| 171/171 [00:27<00:00,  6.18it/s, v_num=5gb8]










































Epoch 99: 100%|██████████| 171/171 [00:27<00:00,  6.20it/s, v_num=5gb8]
Validation DataLoader 0:  95%|█████████▍| 18/19 [00:00<00:00, 19.29it/s]

